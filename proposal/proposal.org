#+TITLE: Iris
#+SUBTITLE: 
#+AUTHOR: Cedric Sirianni, Mithi Jethwa, Lachlan Kermode
#+OPTIONS: toc:nil
#+LATEX_CLASS: acmart
#+LATEX_CLASS_OPTIONS: [sigconf]
#+LATEX_HEADER: \usepackage{hyperref}

#+latex: \hypersetup{ colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan, }



#+BIBLIOGRAPHY: ./references.bib 


* Problem
Vector databases are growing in popularity as they become widely used in image similarity search and RAG systems.
The current approaches for distributed vector databases port existing notions from the distribution of column-based databases (using strategies such as replication and sharding) without taking specific advantage of a vector database's unique architectural features.

* Project Idea

We seek to answer "How do mainstream vector database vector distribution techniques differ in terms of latency and accuracy?" 
Our codebase, nicknamed "Iris", will provide an API to distribute vectors on one or more shards using at least the following techniques:

1) Replication, where all vectors are stored on all nodes, and a master node load-balances new queries. Supported in [[https://qdrant.tech/documentation/guides/distributed_deployment/#replication][Qdrant]], [[https://weaviate.io/developers/weaviate/concepts/replication-architecture][Weaviate]].
2) Random partitioning, where each node contains a distinct set of vectors. An incoming query is sent to all nodes, and results are aggregated and pruned in the user result. Supported in [[https://qdrant.tech/documentation/guides/distributed_deployment/#sharding][Qdrant]], [[https://milvus.io/docs/use-partition-key.md][Milvus]].
3) HNSW-aware sharding, where some number of Voronoi cells is stored on each node. Incoming queries can thus be directed only to those nodes where there are vectors proximate to the query. The HNSW index is stored entirely on the coordinator node [cite:@sunDistributedSystemLarge2024;@dengPyramidGeneralFramework2019;@chenSPANNHighlyefficientBillionscale2021]. 

After evaluating each technique, we will consider areas for optimization.
In particular, we are interested in *semantic caching*, "a method of retrieval optimization where similar queries instantly retrieve the same appropriate response from a knowledge base." [cite:@romeroSemanticCacheQdrant]
As a reach goal, we will implement a cache in the shardcontroller.

* Novelty 

* Implementation 
We first aim to appraise and benchmark the SoTA of vector database distribution.
From a preliminary search of recent literature in vector databases and commercial offerings, we understand there to be three ways in which vector databases have been distributed, listed in [[Project Idea][Project Idea]].
As we review more relevant literature on distribution models for vector databases, we may extend this list to benchmark other models.

Next, we will implement and evaluate each distribution technique in Rust on top of a system such as Qdrant or Faiss. [[https://qdrant.tech/documentation/guides/distributed_deployment/#sharding][Qdrant]] supports two forms of sharding: automatic sharding and user-defined sharding, which roughly correspond to replication and random partitioning, respectively. [[https://qdrant.tech/documentation/guides/distributed_deployment/#user-defined-sharding][User-defined partitioning]] can be extended to HSNW-aware sharding by defining the ~sharding technique~ as ~custom~ and using our own shard key.

TODO: Explain cache

* Resources 

To deploy and evaluate our system, we see three major approaches, each with different tradeoffs:

1) Brown Computing Cluster: Free resource intended for research purposes, but we are unsure about registration eligibility and resource availability.
2) AWS/GCP/Azure: Consumption-based cost model with excellent resource availability and ease-of-use. Can Brown provide credits?
3) Cloudlab: Free, but resource availability seems sparse and usability is inferior to AWS/GCP/Azure.

* Evaluation

Big-ANN and DEEP1B are datasets commonly used in benchmarking.
We can use a "brute-force" approach to compute the objective similarity: for each query vector, compute the similarity with every other vector in the database.
Then, we can compare the "brute-force" result to each distribution technique by counting the number of queries for which the true nearest neighbor is returned first in the result list (a measure called 1-recall@1) or by measuring the average fraction of 10 nearest neighbors that are returned in the first 10 results (the "10-intersection" measure) [CITATION NEEDED, FOUND HERE: https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/]

TODO: How do we measure latency?

* Team members

* Timeline

TODO: Create rough time estimates for engineering hours required for each task. Delegate work to group members (though I imagine we will do a lot of this together).

* Expected challenges

* Bibliography
